use std::{borrow::Cow, cell::RefCell, path::Path};

use criterion::{black_box, criterion_group, criterion_main, Criterion};
use parol_runtime::{
    lexer::tokenizer::{ERROR_TOKEN, UNMATCHABLE_TOKEN},
    once_cell::sync::Lazy,
    ScannerConfig, TerminalIndex, TokenStream, Tokenizer,
};

const LEXER_INPUT: &str = include_str!("./input_1.txt");

// The regex generated by parol for `verly` grammar
const PATTERNS: &[&str] = &[
    /*   0 */ UNMATCHABLE_TOKEN,
    /*   1 */ UNMATCHABLE_TOKEN,
    /*   2 */ UNMATCHABLE_TOKEN,
    /*   3 */ UNMATCHABLE_TOKEN,
    /*   4 */ UNMATCHABLE_TOKEN,
    /*   5 */ r"(//.*(\r\n|\r|\n)|/\*([^*]|\*[^/])*\*/\s*)+",
    /*   6 */
    r"\u{0022}(\\[\u{0022}\\/bfnrt]|u[0-9a-fA-F]{4}|[^\u{0022}\\\u0000-\u001F])*\u{0022}",
    /*   7 */
    r"[0-9]+(_[0-9]+)*\.[0-9]+(_[0-9]+)*[eE][+-]?[0-9]+(_[0-9]+)*",
    /*   8 */ r"[0-9]+(_[0-9]+)*\.[0-9]+(_[0-9]+)*",
    /*   9 */
    r"([0-9]+(_[0-9]+)*)?'s?[bodh][0-9a-fA-FxzXZ]+(_[0-9a-fA-FxzXZ]+)*",
    /*  10 */ r"([0-9]+(_[0-9]+)*)?'[01xzXZ]",
    /*  11 */ r"[0-9]+(_[0-9]+)*",
    /*  12 */ r"\-:",
    /*  13 */ r"\->",
    /*  14 */ r"\+:",
    /*  15 */ r"\+=|-=|\*=|/=|%=|&=|\|=|\^=|<<=|>>=|<<<=|>>>=",
    /*  16 */ r"\*\*",
    /*  17 */ r"/|%",
    /*  18 */ r"\+|-",
    /*  19 */ r"<<<|>>>|<<|>>",
    /*  20 */ r"<=|>=|<:|>:",
    /*  21 */ r"===|==\?|!==|!=\?|==|!=",
    /*  22 */ r"&&",
    /*  23 */ r"\|\|",
    /*  24 */ r"&",
    /*  25 */ r"\^~|\^|~\^",
    /*  26 */ r"\|",
    /*  27 */ r"~&|~\||!|~",
    /*  28 */ r"`",
    /*  29 */ r"::<",
    /*  30 */ r"::",
    /*  31 */ r":",
    /*  32 */ r",",
    /*  33 */ r"\.\.=",
    /*  34 */ r"\.\.",
    /*  35 */ r"\.",
    /*  36 */ r"=",
    /*  37 */ r"\#",
    /*  38 */ r"<",
    /*  39 */ r"'\{",
    /*  40 */ r"\{",
    /*  41 */ r"\[",
    /*  42 */ r"\(",
    /*  43 */ r">",
    /*  44 */ r"\}",
    /*  45 */ r"\]",
    /*  46 */ r"\)",
    /*  47 */ r";",
    /*  48 */ r"\*",
    /*  49 */ r"always_comb",
    /*  50 */ r"always_ff",
    /*  51 */ r"assign",
    /*  52 */ r"as",
    /*  53 */ r"bit",
    /*  54 */ r"case",
    /*  55 */ r"clock",
    /*  56 */ r"clock_posedge",
    /*  57 */ r"clock_negedge",
    /*  58 */ r"const",
    /*  59 */ r"default",
    /*  60 */ r"else",
    /*  61 */ r"embed",
    /*  62 */ r"enum",
    /*  63 */ r"export",
    /*  64 */ r"f32",
    /*  65 */ r"f64",
    /*  66 */ r"final",
    /*  67 */ r"for",
    /*  68 */ r"function",
    /*  69 */ r"i32",
    /*  70 */ r"i64",
    /*  71 */ r"if_reset",
    /*  72 */ r"if",
    /*  73 */ r"import",
    /*  74 */ r"include",
    /*  75 */ r"initial",
    /*  76 */ r"inout",
    /*  77 */ r"input",
    /*  78 */ r"inside",
    /*  79 */ r"inst",
    /*  80 */ r"interface",
    /*  81 */ r"in",
    /*  82 */ r"let",
    /*  83 */ r"logic",
    /*  84 */ r"lsb",
    /*  85 */ r"modport",
    /*  86 */ r"module",
    /*  87 */ r"msb",
    /*  88 */ r"output",
    /*  89 */ r"outside",
    /*  90 */ r"package",
    /*  91 */ r"param",
    /*  92 */ r"proto",
    /*  93 */ r"pub",
    /*  94 */ r"ref",
    /*  95 */ r"repeat",
    /*  96 */ r"reset",
    /*  97 */ r"reset_async_high",
    /*  98 */ r"reset_async_low",
    /*  99 */ r"reset_sync_high",
    /* 100 */ r"reset_sync_low",
    /* 101 */ r"return",
    /* 102 */ r"break",
    /* 103 */ r"signed",
    /* 104 */ r"step",
    /* 105 */ r"string",
    /* 106 */ r"struct",
    /* 107 */ r"switch",
    /* 108 */ r"tri",
    /* 109 */ r"type",
    /* 110 */ r"u32",
    /* 111 */ r"u64",
    /* 112 */ r"union",
    /* 113 */ r"unsafe",
    /* 114 */ r"var",
    /* 115 */ r"\$[a-zA-Z_][0-9a-zA-Z_$]*",
    /* 116 */ r"(r#)?[a-zA-Z_][0-9a-zA-Z_$]*",
    /* 117 */ r"[^{}]*",
    /* 118 */ ERROR_TOKEN,
];

const SCANNER_SPECIFICS: &[&str] = &[
    /*  0 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    /*  1 */ parol_runtime::lexer::tokenizer::NEW_LINE_TOKEN,
    /*  2 */ parol_runtime::lexer::tokenizer::WHITESPACE_TOKEN,
    /*  3 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    /*  4 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
];

const SCANNER_TERMINAL_INDICES: &[TerminalIndex] = &[
    5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
    30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
    54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
    78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,
    101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,
];
const MAX_K: usize = 3;
const ERROR_TOKEN_INDEX: TerminalIndex = 90;

static SCANNERS: Lazy<Vec<ScannerConfig>> = Lazy::new(|| {
    vec![ScannerConfig {
        name: "INITIAL",
        tokenizer: Tokenizer::build(PATTERNS, SCANNER_SPECIFICS, SCANNER_TERMINAL_INDICES).unwrap(),
        transitions: &[],
    }]
});

fn build_scanner() {
    let _tokenizer = black_box(
        Tokenizer::build(PATTERNS, SCANNER_SPECIFICS, SCANNER_TERMINAL_INDICES)
            .expect("Scanner build failed"),
    );
}

fn tokenize() {
    let file_name: Cow<Path> = Path::new("./input_1.txt").to_owned().into();
    let token_stream =
        RefCell::new(TokenStream::new(LEXER_INPUT, file_name, &SCANNERS, MAX_K).unwrap());
    while !token_stream.borrow().all_input_consumed() {
        let tok = token_stream.borrow_mut().lookahead(0).unwrap();
        assert_ne!(tok.token_type, ERROR_TOKEN_INDEX);
        token_stream.borrow_mut().consume().unwrap();
    }
}

fn tokenize_benchmark(c: &mut Criterion) {
    c.bench_function("tokenize", |b| b.iter(tokenize));
}

fn build_scanner_benchmark(c: &mut Criterion) {
    c.bench_function("build_scanner", |b| b.iter(build_scanner));
}

criterion_group!(benchesscanner, tokenize_benchmark);
criterion_group!(benchesbuilder, build_scanner_benchmark);
criterion_main!(benchesscanner, benchesbuilder);
