use std::{borrow::Cow, cell::RefCell, path::Path};

use criterion::{black_box, criterion_group, criterion_main, Criterion};
use parol_runtime::{once_cell::sync::Lazy, ScannerConfig, TerminalIndex, TokenStream, Tokenizer};
use scnr::{ScannerBuilder, ScannerMode};

const LEXER_INPUT: &str = include_str!("./input_1.txt");

// The regex generated by parol for `verly` grammar
const PATTERNS: &[(&str, Option<(bool, &str)>)] = &[
    (parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN, None),
    (parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN, None),
    (parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN, None),
    (parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN, None),
    (parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN, None),
    (
        r"(((//.*(\r\n|\r|\n))|(/\u{2a}[.\r\n]*?\u{2a}/))\s*)+",
        None,
    ),
    (
        r"[0-9]+(_[0-9]+)*\.[0-9]+(_[0-9]+)*[eE][+-]?[0-9]+(_[0-9]+)*",
        None,
    ),
    (r"[0-9]+(_[0-9]+)*\.[0-9]+(_[0-9]+)*", None),
    (
        r"[0-9]+(_[0-9]+)*'[bodh][0-9a-fA-FxzXZ]+(_[0-9a-fA-FxzXZ]+)*",
        None,
    ),
    (r"[0-9]+(_[0-9]+)*", None),
    (r"'[01xzXZ]", None),
    (r"\-:", None),
    (r"\->", None),
    (r"\+:", None),
    (r"\+=|-=|\*=|/=|%=|&=|\|=|\^=|<<=|>>=|<<<=|>>>=", None),
    (r"\*\*", None),
    (r"/|%", None),
    (r"\+|-", None),
    (r"<<<|>>>|<<|>>", None),
    (r"<=|>=|<|>", None),
    (r"===|==\?|!==|!=\?|==|!=", None),
    (r"&&", None),
    (r"\|\|", None),
    (r"&", None),
    (r"\^~|\^|~\^", None),
    (r"\|", None),
    (r"~&|~\||!|~", None),
    (r"::", None),
    (r":", None),
    (r",", None),
    (r"\$", None),
    (r"\.\.", None),
    (r"\.", None),
    (r"=", None),
    (r"\#", None),
    (r"\{", None),
    (r"\[", None),
    (r"\(", None),
    (r"\}", None),
    (r"\]", None),
    (r"\)", None),
    (r";", None),
    (r"\*", None),
    (r"always_comb", None),
    (r"always_ff", None),
    (r"assign", None),
    (r"async_high", None),
    (r"async_low", None),
    (r"as", None),
    (r"bit", None),
    (r"case", None),
    (r"default", None),
    (r"else", None),
    (r"enum", None),
    (r"export", None),
    (r"f32", None),
    (r"f64", None),
    (r"for", None),
    (r"function", None),
    (r"i32", None),
    (r"i64", None),
    (r"if_reset", None),
    (r"if", None),
    (r"import", None),
    (r"inout", None),
    (r"input", None),
    (r"inst", None),
    (r"interface", None),
    (r"in", None),
    (r"localparam", None),
    (r"logic", None),
    (r"modport", None),
    (r"module", None),
    (r"negedge", None),
    (r"output", None),
    (r"package", None),
    (r"parameter", None),
    (r"posedge", None),
    (r"ref", None),
    (r"repeat", None),
    (r"return", None),
    (r"step", None),
    (r"struct", None),
    (r"sync_high", None),
    (r"sync_low", None),
    (r"tri", None),
    (r"u32", None),
    (r"u64", None),
    (r"var", None),
    (r"[a-zA-Z_][0-9a-zA-Z_]*", None),
    (r".", None),
];

const SCANNER_SPECIFICS: &[&str] = &[
    /*  0 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    /*  1 */ parol_runtime::lexer::tokenizer::NEW_LINE_TOKEN,
    /*  2 */ parol_runtime::lexer::tokenizer::WHITESPACE_TOKEN,
    /*  3 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    /*  4 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
];

const SCANNER_TERMINAL_INDICES: &[TerminalIndex] = &[
    5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
    30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
    54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
    78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,
];

const MAX_K: usize = 3;
const ERROR_TOKEN_INDEX: TerminalIndex = 90;

static SCANNERS: Lazy<Vec<ScannerConfig>> = Lazy::new(|| {
    vec![ScannerConfig {
        name: "INITIAL",
        tokenizer: Tokenizer::build(PATTERNS, SCANNER_SPECIFICS, SCANNER_TERMINAL_INDICES).unwrap(),
        transitions: &[],
    }]
});

static USED_MODES: Lazy<Vec<ScannerMode>> = Lazy::new(|| {
    SCANNERS
        .iter()
        .map(|s| s.into())
        .collect::<Vec<ScannerMode>>()
});

fn build_scanner() {
    let _scanner = black_box(
        ScannerBuilder::new()
            .add_scanner_modes(&USED_MODES)
            .build()
            .expect("Scanner build failed"),
    );
}

fn build_nfa_scanner() {
    let _scanner = black_box(
        ScannerBuilder::new()
            .add_scanner_modes(&USED_MODES)
            .use_nfa()
            .build()
            .expect("Scanner build failed"),
    );
}

fn tokenize() {
    let file_name: Cow<Path> = Path::new("./input_1.txt").to_owned().into();
    let token_stream =
        RefCell::new(TokenStream::new(LEXER_INPUT, file_name, &SCANNERS, MAX_K, false).unwrap());
    while !token_stream.borrow().all_input_consumed() {
        let tok = token_stream.borrow_mut().lookahead(0).unwrap();
        assert_ne!(
            tok.token_type, ERROR_TOKEN_INDEX,
            "Error token found: {:?}",
            tok
        );
        token_stream.borrow_mut().consume().unwrap();
    }
}

fn tokenize_nfa() {
    let file_name: Cow<Path> = Path::new("./input_1.txt").to_owned().into();
    let token_stream =
        RefCell::new(TokenStream::new(LEXER_INPUT, file_name, &SCANNERS, MAX_K, true).unwrap());
    while !token_stream.borrow().all_input_consumed() {
        let tok = token_stream.borrow_mut().lookahead(0).unwrap();
        assert_ne!(
            tok.token_type, ERROR_TOKEN_INDEX,
            "Error token found: {:?}",
            tok
        );
        token_stream.borrow_mut().consume().unwrap();
    }
}

fn tokenize_benchmark(c: &mut Criterion) {
    c.bench_function("tokenize", |b| b.iter(tokenize));
}

fn tokenize_nfa_benchmark(c: &mut Criterion) {
    c.bench_function("tokenize_nfa", |b| b.iter(tokenize_nfa));
}

fn build_scanner_benchmark(c: &mut Criterion) {
    c.bench_function("build_scanner", |b| b.iter(build_scanner));
}

fn build_nfa_scanner_benchmark(c: &mut Criterion) {
    c.bench_function("build_nfa_scanner", |b| b.iter(build_nfa_scanner));
}

criterion_group!(benchesscanner, tokenize_benchmark, tokenize_nfa_benchmark);
criterion_group!(
    benchesbuilder,
    build_scanner_benchmark,
    build_nfa_scanner_benchmark
);
criterion_main!(benchesscanner, benchesbuilder);
