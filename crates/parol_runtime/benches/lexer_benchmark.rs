use std::{borrow::Cow, cell::RefCell, path::Path};

use criterion::{black_box, criterion_group, criterion_main, Criterion};
use parol_runtime::{once_cell::sync::Lazy, ScannerConfig, TerminalIndex, TokenStream, Tokenizer};
use scnr::{ScannerBuilder, ScannerMode};

const LEXER_INPUT: &str = include_str!("./input_1.txt");

// The regex generated by parol for `verly` grammar
const PATTERNS: &[&str] = &[
    parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    r"(((//.*(\r\n|\r|\n))|(/\u{2a}[.\r\n]*?\u{2a}/))\s*)+",
    r"[0-9]+(_[0-9]+)*\.[0-9]+(_[0-9]+)*[eE][+-]?[0-9]+(_[0-9]+)*",
    r"[0-9]+(_[0-9]+)*\.[0-9]+(_[0-9]+)*",
    r"[0-9]+(_[0-9]+)*'[bodh][0-9a-fA-FxzXZ]+(_[0-9a-fA-FxzXZ]+)*",
    r"[0-9]+(_[0-9]+)*",
    r"'[01xzXZ]",
    r"\-:",
    r"\->",
    r"\+:",
    r"\+=|-=|\*=|/=|%=|&=|\|=|\^=|<<=|>>=|<<<=|>>>=",
    r"\*\*",
    r"/|%",
    r"\+|-",
    r"<<<|>>>|<<|>>",
    r"<=|>=|<|>",
    r"===|==\?|!==|!=\?|==|!=",
    r"&&",
    r"\|\|",
    r"&",
    r"\^~|\^|~\^",
    r"\|",
    r"~&|~\||!|~",
    r"::",
    r":",
    r",",
    r"\$",
    r"\.\.",
    r"\.",
    r"=",
    r"\#",
    r"\{",
    r"\[",
    r"\(",
    r"\}",
    r"\]",
    r"\)",
    r";",
    r"\*",
    r"always_comb",
    r"always_ff",
    r"assign",
    r"async_high",
    r"async_low",
    r"as",
    r"bit",
    r"case",
    r"default",
    r"else",
    r"enum",
    r"export",
    r"f32",
    r"f64",
    r"for",
    r"function",
    r"i32",
    r"i64",
    r"if_reset",
    r"if",
    r"import",
    r"inout",
    r"input",
    r"inst",
    r"interface",
    r"in",
    r"localparam",
    r"logic",
    r"modport",
    r"module",
    r"negedge",
    r"output",
    r"package",
    r"parameter",
    r"posedge",
    r"ref",
    r"repeat",
    r"return",
    r"step",
    r"struct",
    r"sync_high",
    r"sync_low",
    r"tri",
    r"u32",
    r"u64",
    r"var",
    r"[a-zA-Z_][0-9a-zA-Z_]*",
    r".",
];

const SCANNER_SPECIFICS: &[&str] = &[
    /*  0 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    /*  1 */ parol_runtime::lexer::tokenizer::NEW_LINE_TOKEN,
    /*  2 */ parol_runtime::lexer::tokenizer::WHITESPACE_TOKEN,
    /*  3 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
    /*  4 */ parol_runtime::lexer::tokenizer::UNMATCHABLE_TOKEN,
];

const SCANNER_TERMINAL_INDICES: &[TerminalIndex] = &[
    5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,
    30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
    54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,
    78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,
];

const MAX_K: usize = 3;
const ERROR_TOKEN_INDEX: TerminalIndex = 90;

static SCANNERS: Lazy<Vec<ScannerConfig>> = Lazy::new(|| {
    vec![ScannerConfig {
        name: "INITIAL",
        tokenizer: Tokenizer::build(PATTERNS, SCANNER_SPECIFICS, SCANNER_TERMINAL_INDICES).unwrap(),
        transitions: &[],
    }]
});

static USED_MODES: Lazy<Vec<ScannerMode>> = Lazy::new(|| {
    SCANNERS
        .iter()
        .map(|s| s.into())
        .collect::<Vec<ScannerMode>>()
});

fn build_scanner() {
    let _scanner = black_box(
        ScannerBuilder::new()
            .add_scanner_modes(&USED_MODES)
            .build()
            .expect("Scanner build failed"),
    );
}

fn tokenize_1() {
    let file_name: Cow<Path> = Path::new("./input_1.txt").to_owned().into();
    let token_stream =
        RefCell::new(TokenStream::new(LEXER_INPUT, file_name, &SCANNERS, MAX_K).unwrap());
    while !token_stream.borrow().all_input_consumed() {
        let tok = token_stream.borrow_mut().lookahead(0).unwrap();
        assert_ne!(
            tok.token_type, ERROR_TOKEN_INDEX,
            "Error token found: {:?}",
            tok
        );
        token_stream.borrow_mut().consume().unwrap();
    }
}

fn regex_1_benchmark(c: &mut Criterion) {
    c.bench_function("tokenize_1", |b| b.iter(tokenize_1));
}

fn build_scanner_benchmark(c: &mut Criterion) {
    c.bench_function("build_scanner", |b| b.iter(build_scanner));
}

criterion_group!(benches, regex_1_benchmark, build_scanner_benchmark);
criterion_main!(benches);
